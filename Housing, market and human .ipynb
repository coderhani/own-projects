
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Classification, 25 points) In this exercise, you have to work with a data of past and current employees of a company (’HR data.csv’).Your task is to build a classification model to predict which employee will leave the company and which will continue working (the target column is ’left’, where 1 indicates that the person left the company). You can find more details about the data from https://www.kaggle.com/mfaisalqureshi/hr- analytics-and-job-prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Try to understand the different variables using some basic statistical measures and some visualization tools. As the minimum, calculate mean values for numeric columns, value counts for categorical column, correlation of the columns, and a couple of scatter-plots and box-plots to understand the relationship between the outcome column ’left and other variables’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import keras\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the Data\n",
    "HR_df = pd.read_csv('HR_data.csv', index_col=False)\n",
    "HR_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data overview\n",
    "HR_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The statistical measures of the numeric columns\n",
    "HR_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how many department in this dataset\n",
    "HR_df['Department'].value_counts()\n",
    "# Result: Sales has the highest records in this HR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of which apartment will STAY and LEAVE most\n",
    "sns.countplot(x='Department', hue='left', data= HR_df, palette= 'pastel')\n",
    "plt.xticks(rotation=-45)\n",
    "# Result: Sales department has most people stayed with company but also the one who has the highest number of leaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Count of Salary based on the level\n",
    "HR_df['salary'].value_counts()\n",
    "\n",
    "# Result: 1237 peope having the high salary whereas \n",
    "#          more than 7000 people survive with the low income and \n",
    "#          6446 people having the medium range\n",
    "# This will be a main factor to impact on those deciding to leave the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the relationship between people having level of salary and if they get promotion on last five years and \n",
    "#if that makes them stay or leave\n",
    "sns.barplot(x='salary', y='promotion_last_5years', hue='left', data=HR_df)\n",
    "\n",
    "# So, people who has the highest salary is also the one get the most promotion and they tend to stay\n",
    "# while the people who has the lowest income and lowest promotion want to leave the company most. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, analysize with numerical columns\n",
    "# Check the mean, max and min of satisfaction level\n",
    "\n",
    "print('The highest satisfaction level is', HR_df['satisfaction_level'].max())\n",
    "print('The mean satisfaction level is', HR_df['satisfaction_level'].mean())\n",
    "print('The minimum satisfaction level is', HR_df['satisfaction_level'].min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between 'left' and 'satisfaction level'\n",
    "\n",
    "sns.boxplot(x='left', y='satisfaction_level', data=HR_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, study in the number of project\n",
    "\n",
    "print('the maximum no of project one person has is', HR_df['number_project'].max())\n",
    "print('the medium  no of project one person has is ', HR_df['number_project'].mean())\n",
    "print('the minimum no of project one person has is', HR_df['number_project'].min())\n",
    "print('the total number of project in company is', HR_df['number_project'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the satisfaction level & last evaluation \n",
    "\n",
    "sns.scatterplot(y='last_evaluation', x='satisfaction_level', hue='left', data=HR_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, Study in the number of project\n",
    "\n",
    "print('the maximum no of project one person has is', HR_df['number_project'].max())\n",
    "print('the medium  no of project one person has is ', HR_df['number_project'].mean())\n",
    "print('the minimum no of project one person has is', HR_df['number_project'].min())\n",
    "print('the total number of project in company is', HR_df['number_project'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the Time spend company & Number project\n",
    "sns.boxplot(x='time_spend_company', y='number_project', data=HR_df, hue='left')\n",
    "\n",
    "#Result: the number of people leaving company are the one having:\n",
    "#the number of projects from range 3-5 and \n",
    "#years in company is around 2 years or from 5-6 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As overview, Plot the histogram for numeric columns\n",
    "num_bins = 10\n",
    "HR_df.hist(bins=num_bins, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if there are any missing values in data set:\n",
    "\n",
    "HR_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Matrix\n",
    "corr = HR_df.corr()\n",
    "corr = (corr)\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values, annot=True)\n",
    "\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of dataset again and check two categorical columns which are Department & salary\n",
    "# These two columns are transfered into numerics \n",
    "HR_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the original HR_df to the new HR_df_new with only numeric value\n",
    "HR_df_new=pd.get_dummies(HR_df, columns=['Department', 'salary'])\n",
    "HR_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training and test set (25% test set)\n",
    "# As the column 'left' is the outcome values, it will be the outcome column (X)\n",
    "# the rest of columns will be in the prediction column (Y)\n",
    "X = HR_df_new.drop('left',axis=1)\n",
    "y = HR_df_new['left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train & test set with 25% test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the decision tree\n",
    "HR_tree = DecisionTreeClassifier()\n",
    "#Fit the training data\n",
    "HR_model = HR_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prediction:\n",
    "pred_HR_tree=HR_model.predict(X_test)\n",
    "\n",
    "# Create the classification report for prediction and original test data outcomes\n",
    "print(confusion_matrix(y_test,pred_HR_tree))\n",
    "print(classification_report(y_test,pred_HR_tree))\n",
    "print('Decision Tree accuracy score',accuracy_score(y_test,pred_HR_tree))\n",
    "# If no pruning, the prediction has accuracy score at 0.98 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree.export_text(HR_tree, feature_names = list(X.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Create models by optimizing the parameters of (i) decision trees, (ii) bagging, and (iii) random forest classifiers. What is the best accuracy you can achieve across all the models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train & test set with parameters test size 25% and random 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.25)\n",
    "\n",
    "#(i), Decision trees\n",
    "# Create the (i) decision tree this time with parameter\n",
    "HRdecision_tree= DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Fit training data:\n",
    "HRdecision_tree_model= HRdecision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Create the prediction\n",
    "HRdecision_tree=HRdecision_tree_model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,HRdecision_tree))\n",
    "print(classification_report(y_test,HRdecision_tree))\n",
    "\n",
    "#The number of wrongly predicted cases are 68+34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(ii) Bagging\n",
    "#Create the decision tree place holder:\n",
    "HR_bag= DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Create the bagging classifier object:\n",
    "# I will build 400 different decision trees so n_estimators=400\n",
    "HR_setbag= BaggingClassifier(base_estimator=HR_bag, n_estimators=400)\n",
    "\n",
    "# Fit the training data\n",
    "HR_setbag_classifier= HR_setbag.fit(X_train, y_train)\n",
    "\n",
    "# Create the prediction\n",
    "pred_bag= HR_setbag.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, pred_bag))\n",
    "print(classification_report(y_test, pred_bag))\n",
    "\n",
    "#Result: The number of wrongly predicted cases are 22+31 cases  which is less than decision tree model has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(iii) Random forest: I will still use the 400 trees\n",
    "HR_forest_bag= RandomForestClassifier(n_estimators=400, random_state=0)\n",
    "\n",
    "# Fit the training data:\n",
    "HR_forest_bag.fit(X_train, y_train)\n",
    "\n",
    "# Create the prediction \n",
    "pred_HR_forest= HR_forest_bag.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, pred_HR_forest))\n",
    "print(classification_report(y_test, pred_HR_forest))\n",
    "\n",
    "# Result: random forest shows 8 +32 wrongly predicted cases which is more positive than the above models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding three most important predictors according to the best random forest model\n",
    "pd.Series(HR_forest_bag.feature_importances_, index=X_train.columns)\n",
    "\n",
    "# Result: the most important predictors: satisfaction_level, last evaluation and number project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new random forest model that uses only those three variables as predictors. \n",
    "#What is the best accuracy you can achieve using only the three variables?\n",
    "X=HR_df_new[['satisfaction_level', 'last_evaluation', 'number_project']]\n",
    "y=HR_df_new['left']\n",
    "\n",
    "# Create the trai\n",
    "n & test set with parameters test size:0,25 and random state :42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(iii) Random forest: use the 500 trees\n",
    "New_HR_forest_bag= RandomForestClassifier(n_estimators=500, random_state=0)\n",
    "\n",
    "# Fit the training data:\n",
    "New_HR_forest_bag.fit(X_train, y_train)\n",
    "\n",
    "# Create the prediction \n",
    "Predict_NewHR_forest_bag= New_HR_forest_bag.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, Predict_NewHR_forest_bag))\n",
    "print(classification_report(y_test, Predict_NewHR_forest_bag))\n",
    "# Result: There are 83 + 94 cases are incorrectly predicted, and\n",
    "#With only three variables, the accuracy of the dataset declines compared to when we uses whole other variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data=pd.read_csv('Housing.csv')\n",
    "housing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.info()\n",
    "housing_data.describe()\n",
    "# has numeric values in dataset and there's no missing values\n",
    "# There are 17 columns and 10000 rows in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram for numeric columns\n",
    "num_bins = 10\n",
    "housing_data.hist(bins=num_bins, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which house has the biggest, avaerage and smallest square meter\n",
    "print('the largest square meter is' , housing_data['squareMeters'].max())\n",
    "print('the  average square meter is' , housing_data['squareMeters'].mean())\n",
    "print('the smallest square meter is' , housing_data['squareMeters'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check which is the most expensive, average-price and cheapest house\n",
    "print('the highest price is ', housing_data['price'].max())\n",
    "print('the average price is ', housing_data['price'].mean())\n",
    "print('the minimum price is ', housing_data['price'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between price and square meter\n",
    "sns.lineplot(x='squareMeters', y='price', data=housing_data)\n",
    "# Result: the bigger the house, the higher the price is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next I will see the price according to years (which is made column)\n",
    "housing_data['price'].groupby(housing_data['made']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the relationship between price and square meter and if itäs is newly built or not\n",
    "sns.lineplot(x='made', y='price', data=housing_data, hue='isNewBuilt')\n",
    "# the price is flunctuated thoughout years, the higest price falls 2005 and lowest one falls into aroun 2018 for newly buitt\n",
    "# the highest price for non-newly built fall into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next I want to check iff other facilities might affect the price or not\n",
    "sns.boxplot(x='hasPool', y='price', data=housing_data)\n",
    "# Result: houses having pools or not is not a important to the housing price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between price and if the house has yard or not\n",
    "sns.boxplot(x='hasYard', y='price', data=housing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally I wanted to see the relationship among columns via heatmap\n",
    "plt.figure(figsize = (25, 18))\n",
    "housing_data.corr()\n",
    "sns.heatmap(housing_data.corr(), xticklabels=housing_data.corr().columns.values, \n",
    "            yticklabels=housing_data.corr().columns.values, annot=True, cmap=\"BuPu\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and test set\n",
    "X=housing_data.drop('price', axis=1)\n",
    "y=housing_data['price']\n",
    "# Initialize the training and test set with 20% test size\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the instance of Decision Tree Regressor\n",
    "housing_regressor= DecisionTreeRegressor(random_state=0)\n",
    "# Fit the data\n",
    "housing_tree_fit=housing_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the prediction \n",
    "housing_pred= housing_tree_fit.predict(X_test)\n",
    "\n",
    "# Calculate the MSE\n",
    "housing_MSE=MSE(y_test, housing_pred)\n",
    "\n",
    "print('MSE of housing tree is ', housing_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the paraemeters\n",
    "# Create the instance\n",
    "housing_reg_2=DecisionTreeRegressor(max_depth = 8, min_samples_leaf = 10, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data for new instance\n",
    "housing_tree_fit_2= housing_reg_2.fit(X_train, y_train)\n",
    "y_housing_pred_2=housing_tree_fit_2.predict(X_test)\n",
    "mse_2=MSE(y_test, y_housing_pred_2)\n",
    "\n",
    "print('the new MSE of housing tree is ', mse_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create predictive models of neural network using keras. Experiment with different number of layers (up to 4). What is the best MSE that you can achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Keras\n",
    "\n",
    "model_housing = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, try with the layer with 30 nodes\n",
    "model_housing.add(Dense(30, activation='relu', \n",
    "                        input_shape=(len(X.columns),)))\n",
    "#Second layer with 25 nodes:\n",
    "model_housing.add(Dense(25, activation='relu'))\n",
    "#Third layer with 20 nodes:\n",
    "model_housing.add(Dense(20, activation='relu'))\n",
    "#Fourth layer with 15 nodes:\n",
    "model_housing.add(Dense(15, activation='relu'))\n",
    "\n",
    "model_housing.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, Fit the model\n",
    "\n",
    "model_housing_fit = model_housing.fit(X_train, y_train, epochs=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the best MSE that you can achieve?\n",
    "\n",
    "losses=model_housing_fit.history['loss']\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_MSE=pd.DataFrame(housing_model_fit.history).min()\n",
    "print(' The best MSE can be achieved is', best_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import keras\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = pd.read_csv('text_data.csv')\n",
    "data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text['text']=data_text['text'].str.lower()\n",
    "data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text['text_tokenized']=data_text.text.apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# the columns are extracted for text analysis\n",
    "data_set=data_text[['text', 'text_tokenized']]\n",
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the stopwords in english\n",
    "stop_words= list(stopwords.words('english'))\n",
    "\n",
    "# Remove the stop words in the text_tokenized\n",
    "data_set['text_token_split']=data_set['text_tokenized'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for stemming result\n",
    "text_set['stemmed']=text_set['text_token_split'].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_set['text_token_split'].apply(lambda x: [stemmer.stem(y) for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original text:\n",
    "print(data_set.text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second iteration with lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the object\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer_net(text):\n",
    "    text_lemm=[lemmatizer.lemmatize(word) for word in text]\n",
    "    return text_lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set['text_lemmatized']= data_set['text_token_split'].apply(lambda x: lemmatizer_net(x))\n",
    "print(data_set['text_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the result\n",
    "data_set\n",
    "# Result: after the second iteration, stopwords have been removed while the text becomes informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the text_lemmatized column into the array of string\n",
    "data_set['text_final']= data_set['text_lemmatized'].apply(lambda x:' '.join(x))\n",
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Initalize an object\n",
    "data_vec= CountVectorizer()\n",
    "#Create the representation\n",
    "data_counts=text_vec.fit_transform(data_set['text_final'])\n",
    "data_counts_dataframe=pd.DataFrame(data_counts.toarray(), \n",
    "                                   columns=text_vec.get_feature_names())\n",
    "data_counts_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top frequent 15 words, is\n",
    "top15_frequent_words= data_counts_dataframe.sum(axis=0\n",
    "                                         ).sort_values(ascending=False)[:15]\n",
    "top15_frequent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#Create the itineration:\n",
    "data_LDA= LatentDirichletAllocation(n_components=3, random_state=42)\n",
    "\n",
    "#Fit the text_counts that are created for the CountVectorizer\n",
    "data_LDA_results=data_LDA.fit_transform(data_counts)\n",
    "data_LDA.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top important words for each topic\n",
    "for topic, text_component in enumerate(data_LDA.components_):\n",
    "    #Sorted the top 15 importants in each topic:\n",
    "    top15_important_words= np.argsort(text_component)[-15:]\n",
    "    \n",
    "    print([text_vec.get_feature_names()[i] for i in top15_important_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_LDA_results\n",
    "# Result: the first article is showned in the third topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the connection between the three original news categories and the three extracted topics\n",
    "text_LDA_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the column text_topic to see each article in each topic\n",
    "data_set['text_topic']=text_LDA_results.argmax(axis=1)\n",
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "airbnb=pd.read_csv('new_york_airbnb.csv.csv')\n",
    "airbnb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(airbnb.neighbourhood_group.unique())\n",
    "print(airbnb.neighbourhood.unique())\n",
    "print(airbnb.room_type.unique())\n",
    "print(airbnb.last_review.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I will drop some columns as request\n",
    "new_airbnb=airbnb.drop(['id','host_name','name','host_id','neighbourhood', 'last_review'], axis='columns')#, inplace=True)\n",
    "new_airbnb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the value which is missing in new dataset\n",
    "new_airbnb.isnull().sum()\n",
    "# Result: 10052 missing value in the review_per_month column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill N/A:\n",
    "new_airbnb.fillna({'reviews_per_month':0}, inplace=True)\n",
    "new_airbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn the categorical column into dummy\n",
    "new_airbnb=pd.concat([new_airbnb,pd.get_dummies(\n",
    "    new_airbnb['neighbourhood_group'])],axis=1)\n",
    "\n",
    "new_airbnb=pd.concat([new_airbnb,pd.get_dummies(\n",
    "    new_airbnb['room_type'])],axis=1)\n",
    "\n",
    "new_airbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new dataset trong random forest modelling\n",
    "data_rental=new_airbnb.drop(['room_type', 'neighbourhood_group'], axis='columns')\n",
    "\n",
    "data_rental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlation among columns\n",
    "# Build a prediction model for predicting price with random forest regression \n",
    "\n",
    "plt.figure(figsize = (25, 20))\n",
    "sns.heatmap(data_rental.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the training and test set\n",
    "X=data_rental.drop('price', axis='columns')\n",
    "y=data_rental['price']\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a random forest placeholder\n",
    "forest_rental=RandomForestClassifier(random_state=42)\n",
    "credit=[25, 50, 100]\n",
    "features=['auto', 'sqrt']\n",
    "max_depth= [2,4,5,8,10,12]\n",
    "criterion=['gini', 'entropy']\n",
    "weights = ['balanced', {0:0.1, 1:0.9}]\n",
    "#define the grid\n",
    "grid_rental=dict(n_estimators=credit, criterion=criterion, max_depth=max_depth,\n",
    "               max_features=features, class_weight=weights )\n",
    "grid_rental_search=GridSearchCV(estimator=forest_rental,cv = 2,\n",
    "                        n_jobs = 5, param_grid=grid_rental, scoring='accuracy')\n",
    "result_rental=grid_rental_search.fit(X,y)\n",
    "print('Result is', result_rental.best_score_,\n",
    "     'using', result_rental.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random forest classifier model with above result:\n",
    "final_forest_rental=RandomForestClassifier(n_estimators=100, class_weight= 'balanced', \n",
    "                criterion= 'entropy', max_depth= 12, max_features= 'auto', random_state = 0)\n",
    "\n",
    "# Fit the training data:\n",
    "final_forest_rental.fit(X_train, y_train)\n",
    "\n",
    "# Predict the results\n",
    "prediction_rental=final_forest_rental.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, prediction_rental))\n",
    "print(classification_report(y_test, prediction_rental))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the new data set with only column name\n",
    "airbnb=pd.read_csv('new_york_airbnb.csv.csv')\n",
    "airbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data whether has any missing values\n",
    "airbnb.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values in column name with a letter\n",
    "airbnb['name'].fillna(\"$\",inplace=True)\n",
    "airbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the name into lower capitals:\n",
    "airbnb['name']=airbnb.name.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the tokenized column \n",
    "airbnb['name_tokenized']=airbnb['name'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# Columns needed for text analysis are Extracted\n",
    "text_airbnb=airbnb[['name', 'name_tokenized']]\n",
    "text_airbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the stopwords in english\n",
    "stop_words= list(stopwords.words('english'))\n",
    "\n",
    "# Remove the stop words in the text_tokenized\n",
    "text_airbnb['name_token_split']=text_airbnb['name_tokenized'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "text_airbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for stemming result\n",
    "text_airbnb['stemmed']=text_airbnb['name_token_split'].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The result is:\n",
    "print(text_airbnb['name_token_split'].apply(lambda x: [stemmer.stem(y) for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For informative word which are not showed in result,  second initeration is neccessary\n",
    "# The object is initialized\n",
    "\n",
    "rental_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizer_net(text):\n",
    "    lemm_rental=[rental_lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemm_rental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a column after being lemmatized\n",
    "text_airbnb['name_lemmatized']= text_airbnb['name_token_split'].apply(\n",
    "lambda x: lemmatizer_net(x))\n",
    "print(text_airbnb['name_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_airbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text_lemmatized column is returned into the array of string\n",
    "text_airbnb['name_final']= text_airbnb['name_lemmatized'].apply(lambda x:' '.join(x))\n",
    "text_airbnb\n",
    "# Result: The stopwords are removed in the name_final column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the frequent words:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initalize an object\n",
    "name_vect= CountVectorizer()\n",
    "\n",
    "# Create the representation\n",
    "namecounts=name_vect.fit_transform(text_airbnb['name_final'])\n",
    "dataframe_namecounts=pd.DataFrame(namecounts.toarray(), columns=name_vect.get_feature_names())\n",
    "dataframe_namecounts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top frequent 15 word in names\n",
    "top15_frequentname= dataframe_namecounts.sum(axis=0\n",
    "                                         ).sort_values(ascending=False)[:15]\n",
    "top15_frequentname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the stopword list with some common words, like room,..\n",
    "stop_words_new_list=list(stopwords.words(\"english\")) + ['room']+['apartment']+['manhattan'] +['williamsburg']+['brooklyn']\n",
    "\n",
    "# New form\n",
    "new_vectname=CountVectorizer(stop_words = stop_words_new_list)\n",
    "new_countsname= new_vectname.fit_transform(text_airbnb['name_final'])\n",
    "\n",
    "# Convert to dataframe:\n",
    "new_text_airbnb_df= pd.DataFrame(new_countsname.toarray(), columns=new_vectname.get_feature_names())\n",
    "\n",
    "# The new 15 frquent words in name:\n",
    "new_frequentname=new_text_airbnb_df.sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "print(new_frequentname[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
